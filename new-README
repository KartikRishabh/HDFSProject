The key to this application is going to be in small modifications in the
scheduler as well as  more drastic modifications in the NameNode and 
DataNode classes.  

There are some interesting aspects about HDFS. One of them is that at the
file level you can specify different block sizes. The blocks can actually 
change such that the last block of the file may be smaller so that there 
is no wasted space in the blocks. 

Additionally, requesting blocks from a file is rather interesting. 

	When an application reads a file, the HDFS client first asks the NameNode
	for the list of DataNodes that host replicas of the blocks of the file. 
	The list is sorted by the network topology distance from the client. 
	The client contacts a DataNode directly and requests the transfer of the
	desired block. When a client writes, it first asks the NameNode to 
	choose DataNodes to host replicas of the first block of the file. 
	The client organizes a pipeline from node-to-node and sends the data. 
	When the first block is filled, the client requests new DataNodes to 
	be chosen to host replicas of the next block. A new pipeline is organized,
 	and the client sends the further bytes of the file. Choice of DataNodes 
	for each block is likely to be different. 

We'll have to modify the system such that we can ensure that when we write, 
we write blocks in a fashion that ensure that we're doing so to a node that 
has the correct block size. We probably will not support writes in the short
term. 

We also need to disable checksums:

	HDFS generates and stores checksums for each data block of an HDFS file. 
	Checksums are verified by the HDFS client while reading to help detect any
	corruption caused either by client, DataNodes, or network. When a client 
	creates an HDFS file, it computes the checksum sequence for each block and
	sends it to a DataNode along with the data. A DataNode stores checksums in
	a metadata file separate from the block's data file. When HDFS reads a 
	file, each block's data and checksums are shipped to the client. The 
	client computes the checksum for the received data and verifies that the 
	newly computed checksums matches the checksums it received. If not, the 
	client notifies the NameNode of the corrupt replica and then fetches a 
	different replica of the block from another DataNode.

No configuration script will be necessary for us to provide the FS with our network topology:

	HDFS allows an administrator to configure a script that returns a node's 
	rack identification given a node's address. The NameNode is the central 
	place that resolves the rack location of each DataNode. When a DataNode 
	registers with the NameNode, the NameNode runs the configured script to 
	decide which rack the node belongs to. If no such a script is configured, 
	the NameNode assumes that all the nodes belong to a default single rack.

Because everything is done at the block level, we'll have to figure out how 
we want to architect the system. It seems like it may not make sense to 
pretend that each of these blocks belong to the same file; rather, we may 
consider duplicating our file with alternate block sizes and then figure out
how to direct the client application's reads to the right files afterward. 
